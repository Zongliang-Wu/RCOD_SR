
<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Realism Control One-step Diffusion for Real-World Image Super-Resolution">
  <meta name="keywords" content="Real-World Image Super-Resolution, One-step Diffusion">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Realism Control One-step Diffusion for Real-World Image Super-Resolution</title>

    <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/SR/diff_time_step_principle_01.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>




<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Realism Control One-step Diffusion for Real-World Image Super-Resolution</h1>
          
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="mhttps://scholar.google.com/citations?user=fiylCeQAAAAJ">Zongliang Wu</a><sup>1, 2, 3, *</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=zfYLLggAAAAJ">Siming Zheng</a><sup>3, *</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com.hk/citations?hl=zh-CN&user=85QJ_i4AAAAJ">Peng-Tao Jiang</a><sup>3, #</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?hl=en&user=cS9CbWkAAAAJ">Xin Yuan</a><sup>2, #</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup> Zhejiang University, Hangzhou, China</span><br>
            <span class="author-block"><sup>2</sup> School of Engineering, Westlake University, Hangzhou, China</span><br>
            <span class="author-block"><sup>3</sup> vivo Mobile Communication Co., Ltd</span><br>
            <span class="author-block"><sup>*</sup> Equal Contribution, <sup>#</sup> Corresponding Author</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
                            <span class="link-block">
                <a href="https://arxiv.org/abs/2509.10122"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2509.10122"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
                            <span class="link-block">
                <a href="https://github.com/Zongliang-Wu/RCOD"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/videos/teaser.mp4"
                type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        <span class="dnerf">Nerfies</span> turns selfie videos from your phone into
        free-viewpoint
        portraits.
      </h2>
    </div>
  </div>
</section> -->



<section class="section">
  <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Pre-trained diffusion models have shown great potential in real-world image super-resolution (Real-ISR) tasks by enabling high-resolution reconstructions. While one-step diffusion (OSD) methods significantly improve efficiency compared to traditional multi-step approaches, they still have limitations in balancing fidelity and realism across diverse scenarios.
            Since the OSDs for SR are usually trained or distilled by a single timestep, they lack flexible control mechanisms to adaptively prioritize these competing objectives, which are inherently manageable in multi-step methods through adjusting sampling steps. To address this challenge, we propose a **Realism Controlled One-step Diffusion (RCOD)** framework for Real-ISR. RCOD provides a latent domain grouping strategy that enables explicit control over fidelity-realism trade-offs during the noise prediction phase with minimal training paradigm modifications and original training data. A degradation-aware sampling strategy is also introduced to align distillation regularization with the grouping strategy and enhance the controlling of trade-offs.
            Moreover, a visual prompt injection module is used to replace conventional text prompts with degradation-aware visual tokens, enhancing both restoration accuracy and semantic consistency.
            Our method achieves superior fidelity and perceptual quality while maintaining computational efficiency. Extensive experiments demonstrate that RCOD outperforms state-of-the-art OSD methods quantitatively and visually, with flexible realism control capabilities in the inference stage.
          </p>
        </div>
      </div>
    </div>
      </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <div style="text-align: center;">
<!--       <h2 class="title is-3">Visual Results</h2> -->
      <br>
    </div>

    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <!-- <h3 class="title is-4">Fidelity-Realism Trade-off Control</h3> -->
                <img src="./static/images/SR/diff_time_step_principle_01.png" alt="Comparison of RCOD with S3Diff showing control over fidelity and realism." style="max-width: 100%; height: auto;">
        
        <div class="content has-text-justified">
          <p>
           Fig. 1 Realism control one-step diffusion (RCOD) training process. The left part illustrates several synthesized real-world LR images by applying diverse degradations with varying types and intensities on an HR image. (a) Existing vanilla one-step diffusion (OSD) methods for super-resolution (SR): These LR images are directly sent into the diffusion forward and reverse process; the denoising U-Net tends to learn to recover the `average' degradation, leading to a monotonous generation ability within the latent domain. (b) Our proposed Realism Control One-Step Diffusion employs a latent domain grouping strategy. This allows for adaptive control of timesteps (denoising degrees) during the forward process according to the degradation degree in the latent domain. As a result, the denoising U-Net can acquire a more diverse generation capability based on the timestep.          </p>
        </div>
      </div>
    </div>
    <br><br>
  
</section>




<section class="section">
  <div class="container is-max-desktop">
    <div style="text-align: center;">
      <h2 class="title is-3">Visual Results</h2>
      <br>
    </div>

    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h3 class="title is-4">Fidelity-Realism Trade-off Control</h3>
                <img src="./static/images/SR/fig1.png" alt="Comparison of RCOD with S3Diff showing control over fidelity and realism." style="max-width: 200%; height: auto;">
        
        <div class="content has-text-justified">
          <p>
            Fig. 2 While previous one-step diffusion methods, such as S3Diff, only yield one optimal result, our approach offers the flexibility to control images with different fidelity-realism trade-offs during inference. For instance, our method can generate an image optimized for high **Fidelity (Ours-Fid.)** (PSNR: 26.94) or an image optimized for high **Realism (Ours-Real.)** (NIQE: 3.913), adapting to diverse user requirements.
          </p>
        </div>
      </div>
    </div>
    <br><br>
    
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h3 class="title is-4">Qualitative Comparison</h3>
        <img src="./static/images/SR/fig5.png" alt="Qualitative comparison of image Super-Resolution results on various datasets." style="max-width: 100%; height: auto;">
        <div class="content has-text-justified">
          <p>
            Fig. 3 Qualitative comparisons demonstrate that our method consistently achieves superior restoration accuracy and semantic consistency compared to state-of-the-art one-step diffusion methods.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{wu2025realism,
  title={Realism Control One-step Diffusion for Real-World Image Super-Resolution},
  author={Wu, Zongliang and Zheng, Siming and Jiang, Peng-Tao and Yuan, Xin},
  journal={arXiv preprint arXiv:2509.10122},
  year={2025}
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/Zongliang-Wu/RCOD" class="external-link">
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/Zongliang-Wu/RCOD">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
```
